{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have editted the week 2 document that you had started and added in some things.\n",
    "\n",
    "First things are needs for all weeks exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/emmastoklundlee/opt/anaconda3/lib/python3.9/site-packages (4.33.2)\n",
      "Requirement already satisfied: requests in /Users/emmastoklundlee/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/emmastoklundlee/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/emmastoklundlee/opt/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /Users/emmastoklundlee/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/emmastoklundlee/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/emmastoklundlee/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/emmastoklundlee/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/emmastoklundlee/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/emmastoklundlee/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.17.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/emmastoklundlee/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: fsspec in /Users/emmastoklundlee/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2022.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/emmastoklundlee/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/emmastoklundlee/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/emmastoklundlee/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/emmastoklundlee/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/emmastoklundlee/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/emmastoklundlee/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble \n",
    "import sys \n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/emmastoklundlee/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd57e9df25b84e848971b5c5fd1acef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Milloin Charles Fort syntyi?</td>\n",
       "      <td>Charles Fort</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [18], 'answer_text': ['6. elo...</td>\n",
       "      <td>Charles Hoy Fort (6. elokuuta (joidenkin lähte...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Charles%20Fort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“ダン” ダニエル・ジャドソン・キャラハンの出身はどこ</td>\n",
       "      <td>ダニエル・J・キャラハン</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [35], 'answer_text': ['カリフォルニ...</td>\n",
       "      <td>“ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?</td>\n",
       "      <td>వేప</td>\n",
       "      <td>telugu</td>\n",
       "      <td>{'answer_start': [12], 'answer_text': ['Azadir...</td>\n",
       "      <td>వేప (లాటిన్ Azadirachta indica, syn. Melia aza...</td>\n",
       "      <td>https://te.wikipedia.org/wiki/%E0%B0%B5%E0%B1%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>চেঙ্গিস খান কোন বংশের রাজা ছিলেন ?</td>\n",
       "      <td>চেঙ্গিজ খান</td>\n",
       "      <td>bengali</td>\n",
       "      <td>{'answer_start': [414], 'answer_text': ['বোরজি...</td>\n",
       "      <td>চেঙ্গিজ খান (মঙ্গোলীয়: Чингис Хаан  আ-ধ্ব-ব: ...</td>\n",
       "      <td>https://bn.wikipedia.org/wiki/%E0%A6%9A%E0%A7%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>రెయ్యలగడ్ద గ్రామ విస్తీర్ణత ఎంత?</td>\n",
       "      <td>రెయ్యలగడ్ద</td>\n",
       "      <td>telugu</td>\n",
       "      <td>{'answer_start': [259], 'answer_text': ['27 హె...</td>\n",
       "      <td>రెయ్యలగడ్ద, విశాఖపట్నం జిల్లా, గంగరాజు మాడుగుల...</td>\n",
       "      <td>https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B1%...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            question_text document_title  language  \\\n",
       "0            Milloin Charles Fort syntyi?   Charles Fort   finnish   \n",
       "1             “ダン” ダニエル・ジャドソン・キャラハンの出身はどこ   ダニエル・J・キャラハン  japanese   \n",
       "2  వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?            వేప    telugu   \n",
       "3      চেঙ্গিস খান কোন বংশের রাজা ছিলেন ?    চেঙ্গিজ খান   bengali   \n",
       "4        రెయ్యలగడ్ద గ్రామ విస్తీర్ణత ఎంత?     రెయ్యలగడ్ద    telugu   \n",
       "\n",
       "                                         annotations  \\\n",
       "0  {'answer_start': [18], 'answer_text': ['6. elo...   \n",
       "1  {'answer_start': [35], 'answer_text': ['カリフォルニ...   \n",
       "2  {'answer_start': [12], 'answer_text': ['Azadir...   \n",
       "3  {'answer_start': [414], 'answer_text': ['বোরজি...   \n",
       "4  {'answer_start': [259], 'answer_text': ['27 హె...   \n",
       "\n",
       "                                  document_plaintext  \\\n",
       "0  Charles Hoy Fort (6. elokuuta (joidenkin lähte...   \n",
       "1  “ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...   \n",
       "2  వేప (లాటిన్ Azadirachta indica, syn. Melia aza...   \n",
       "3  চেঙ্গিজ খান (মঙ্গোলীয়: Чингис Хаан  আ-ধ্ব-ব: ...   \n",
       "4  రెయ్యలగడ్ద, విశాఖపట్నం జిల్లా, గంగరాజు మాడుగుల...   \n",
       "\n",
       "                                        document_url  \n",
       "0       https://fi.wikipedia.org/wiki/Charles%20Fort  \n",
       "1  https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...  \n",
       "2  https://te.wikipedia.org/wiki/%E0%B0%B5%E0%B1%...  \n",
       "3  https://bn.wikipedia.org/wiki/%E0%A6%9A%E0%A7%...  \n",
       "4  https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B1%...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]\n",
    "\n",
    "df_train = train_set.to_pandas()\n",
    "df_val = validation_set.to_pandas()\n",
    "\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4779\n",
      "29598\n",
      "11394\n"
     ]
    }
   ],
   "source": [
    "# Get train and validation data for each language\n",
    "df_train_bengali = df_train[df_train['language'] == 'bengali']\n",
    "df_train_arabic = df_train[df_train['language'] == 'arabic']\n",
    "df_train_indonesian = df_train[df_train['language'] == 'indonesian']\n",
    "\n",
    "df_val_bengali = df_val[df_val['language'] == 'bengali']\n",
    "df_val_arabic = df_val[df_val['language'] == 'arabic']\n",
    "df_val_indonesian = df_val[df_val['language'] == 'indonesian']\n",
    "\n",
    "print(len(df_train_bengali))\n",
    "print(len(df_train_arabic))\n",
    "print(len(df_train_indonesian))\n",
    "\n",
    "# For testing\n",
    "df_val_english = df_val[df_val['language'] == 'english']\n",
    "df_train_english = df_train[df_train['language'] == 'english']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_bengali_document = df_train[df_train['language'] == 'bengali'][\"document_plaintext\"]\n",
    "df_train_arab_document = df_train[df_train['language'] == 'arabic'][\"document_plaintext\"]\n",
    "df_train_indonesian_document = df_train[df_train['language'] == 'indonesian'][\"document_plaintext\"]\n",
    "df_train_indonesian_document.head()\n",
    "\n",
    "df_train_english_document = df_train[df_train['language'] == 'english'][\"document_plaintext\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_1216/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_1216/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_1216/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_1216/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_1216/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_1216/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_1216/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_1216/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the documents\n",
    "from transformers import AutoTokenizer\n",
    "mbert_tokeniser = AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n",
    "def tokenize(df, key, transformer_model):\n",
    "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
    "\n",
    "# Tokinize train document_plaintext\n",
    "tokenize(df_train_bengali, \"document_plaintext\", mbert_tokeniser)\n",
    "tokenize(df_train_arabic, \"document_plaintext\", mbert_tokeniser)\n",
    "tokenize(df_train_indonesian, \"document_plaintext\", mbert_tokeniser)\n",
    "\n",
    "# Tokinize validation document_plaintext\n",
    "tokenize(df_val_bengali, \"document_plaintext\", mbert_tokeniser)\n",
    "tokenize(df_val_arabic, \"document_plaintext\", mbert_tokeniser)\n",
    "tokenize(df_val_indonesian, \"document_plaintext\", mbert_tokeniser)\n",
    "\n",
    "\n",
    "# For testing\n",
    "tokenize(df_train_english, \"document_plaintext\", mbert_tokeniser)\n",
    "tokenize(df_val_english, \"document_plaintext\", mbert_tokeniser)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have moved tokenization to here, and # it out where it was before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_1216/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_1216/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_1216/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_1216/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_1216/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_1216/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n"
     ]
    }
   ],
   "source": [
    "# added in tokenization of the questions\n",
    "# Tokinize train question_text\n",
    "tokenize(df_train_bengali, \"question_text\", mbert_tokeniser)\n",
    "tokenize(df_train_arabic, \"question_text\", mbert_tokeniser)\n",
    "tokenize(df_train_indonesian, \"question_text\", mbert_tokeniser)\n",
    "\n",
    "# Tokinize validation question_text\n",
    "tokenize(df_val_bengali, \"question_text\", mbert_tokeniser)\n",
    "tokenize(df_val_arabic, \"question_text\", mbert_tokeniser)\n",
    "tokenize(df_val_indonesian, \"question_text\", mbert_tokeniser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data document_plaintext tokenized\n",
    "document_plaintext_tokenized_bengali = list(df_train_bengali[\"document_plaintext_tokenized\"].explode())\n",
    "document_plaintext_tokenized_arabic = list(df_train_arabic[\"document_plaintext_tokenized\"].explode())\n",
    "document_plaintext_tokenized_indonesian = list(df_train_indonesian[\"document_plaintext_tokenized\"].explode())\n",
    "\n",
    "# Validation data document_plaintext tokenized\n",
    "document_plaintext_tokenized_val_bengali = list(df_val_bengali[\"document_plaintext_tokenized\"].explode())\n",
    "document_plaintext_tokenized_val_arabic = list(df_val_arabic[\"document_plaintext_tokenized\"].explode())\n",
    "document_plaintext_tokenized_val_indonesian = list(df_val_indonesian[\"document_plaintext_tokenized\"].explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data question_text tokenized\n",
    "question_text_tokenized_bengali = list(df_train_bengali[\"question_text_tokenized\"].explode())\n",
    "question_text_tokenized_arabic = list(df_train_arabic[\"question_text_tokenized\"].explode())\n",
    "question_text_tokenized_indonesian = list(df_train_indonesian[\"question_text_tokenized\"].explode())\n",
    "\n",
    "# Validation data question_text tokenized\n",
    "question_text_tokenized_val_bengali = list(df_val_bengali[\"question_text_tokenized\"].explode())\n",
    "question_text_tokenized_val_arabic = list(df_val_arabic[\"question_text_tokenized\"].explode())\n",
    "question_text_tokenized_val_indonesian = list(df_val_indonesian[\"question_text_tokenized\"].explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the below is stuff I have added in - to be added into a combined version "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 1 exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) For each of the languages Arabic, Bengali and Indonesian, report the 5 most common words in the documents from the training set. Then report the 5 most common words in the questions from the training set. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def top_5_words(word_list):\n",
    "    word_count = Counter(word_list)\n",
    "    top_5 = word_count.most_common(5)\n",
    "    return top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Bengali words: [('।', 26347), ('##র', 20077), ('স', 19419), ('##ি', 18307), ('##ে', 17732)]\n",
      "Top 5 Arabic words: [('ال', 251106), ('و', 125357), ('،', 106405), ('.', 98368), ('في', 93610)]\n",
      "Top 5 Indonesian words: [(',', 55825), ('.', 47977), ('yang', 24459), ('dan', 24016), ('di', 19190)]\n"
     ]
    }
   ],
   "source": [
    "# can be printed in a better format?\n",
    "print('Top 5 Bengali words:', top_5_words(document_plaintext_tokenized_bengali))\n",
    "print('Top 5 Arabic words:', top_5_words(document_plaintext_tokenized_arabic))\n",
    "print('Top 5 Indonesian words:', top_5_words(document_plaintext_tokenized_indonesian))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Bengali words: [('?', 4777), ('ক', 3552), ('##র', 1914), ('##া', 1625), ('স', 1558)]\n",
      "Top 5 Arabic words: [('؟', 29576), ('ال', 18018), ('م', 10609), ('ما', 8139), ('##تى', 7138)]\n",
      "Top 5 Indonesian words: [('?', 11368), ('apa', 3791), ('##kah', 2783), ('kap', 2339), ('##an', 2185)]\n"
     ]
    }
   ],
   "source": [
    "print('Top 5 Bengali words:', top_5_words(question_text_tokenized_bengali))\n",
    "print('Top 5 Arabic words:', top_5_words(question_text_tokenized_arabic))\n",
    "print('Top 5 Indonesian words:', top_5_words(question_text_tokenized_indonesian))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) implement an “oracle” function that indicates whether a question is an- swerable or not given the document and answer. That is, the function will output 1 if the answer to the question appears in the document and 0 otherwise. Then implement a rule-based classifier that predicts whether a question is answerable only using the document and question. Use the oracle function to evaluate it. What is the performance of your classifier on the validation set for each of the languages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oracle function which takes a dataframe and row of a dataframe to check whether the text of the question appears in the document text\n",
    "def oracle(df, row):\n",
    "    \"\"\"\n",
    "    If text (a word) from question appears in document, assume that question is answerable\n",
    "    Return 1 if answerable\n",
    "    Return 0 if not answerable\n",
    "    \"\"\"\n",
    "    \n",
    "    question = df['question_text'][row].split()\n",
    "    document = df['document_plaintext'][row].split()\n",
    "    \n",
    "    found = False\n",
    "    for word in question:\n",
    "        if word in document:\n",
    "            found = True\n",
    "            break \n",
    "\n",
    "    if found:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a column with whether the oracle function classifies the result as 0 or 1\n",
    "answer_classification = []\n",
    "\n",
    "for index, row in df_train.iterrows():\n",
    "    result = oracle(df_train, index) \n",
    "    answer_classification.append(result)\n",
    "    \n",
    "df_train['answer_classification'] = answer_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a binary column where if the question is answered it is equal to 1, and if not answerable it is 0\n",
    "def check_annotations(annotation):\n",
    "    return annotation == {'answer_start': [-1], 'answer_text': ['']}\n",
    "\n",
    "df_train['correct_answer'] = df_train['annotations'].apply(check_annotations)\n",
    "df_train['correct_answer'] = (~df_train['correct_answer']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics function\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def performance_metrics(df):\n",
    "    y_true = df['correct_answer']\n",
    "    y_pred = df['answer_classification']\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-score\": f1}\n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update other languages dataframes\n",
    "df_train_bengali = df_train[df_train['language'] == 'bengali']\n",
    "df_train_arabic = df_train[df_train['language'] == 'arabic']\n",
    "df_train_indonesian = df_train[df_train['language'] == 'indonesian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'Accuracy': 0.6483151972567569, 'Precision': 0.6254913378948901, 'Recall': 0.740023079970375, 'F1-score': 0.677954066698751}\n",
      "Bengali: {'Accuracy': 0.686336053567692, 'Precision': 0.6260968015850552, 'Recall': 0.9255230125523013, 'F1-score': 0.746918791153132}\n",
      "Arabic: {'Accuracy': 0.6703493479289141, 'Precision': 0.617122969837587, 'Recall': 0.8982776089159068, 'F1-score': 0.7316187594553707}\n",
      "Indonesian: {'Accuracy': 0.6614885027207302, 'Precision': 0.6140437631351218, 'Recall': 0.8710978603998597, 'F1-score': 0.7203248495395548}\n"
     ]
    }
   ],
   "source": [
    "# display performance metrics\n",
    "print('Overall:', performance_metrics(df_train))\n",
    "print('Bengali:', performance_metrics(df_train_bengali))\n",
    "print('Arabic:', performance_metrics(df_train_arabic))\n",
    "print('Indonesian:', performance_metrics(df_train_indonesian))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## above is the end of my part for the first weeks exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Week 2\n",
    "\n",
    "Let k be the number of members in your group (k ∈ {1,2,3}). Implement k different7 language models for each of the three languages, separately for the questions and the documents (total k×3×2 language models), using the training data. Evaluate each of them on the validation data, report their performance and discuss the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, models created for the document texts. This includes: UniformLM, Unigram, Unigram with Laplace smoothing, Bigram with Laplace smoothing and Trigram with Laplace smoothing.\n",
    "\n",
    "Laplace smoothing was added to N-gram models where N>1 as an error occurred meaning that perplexity was infinite. **add explanation of why this was**\n",
    "\n",
    "**We only need to keep 3 of the following 5 models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali perplexity UniformLM: 3623.9999999845454\n",
      "Arabic perplexity UniformLM: 11174.000000245973\n",
      "Indonesian perplexity UniformLM: 18974.000000269545\n"
     ]
    }
   ],
   "source": [
    "from statnlpbook.lm import NGramLM, UniformLM, inject_OOVs, perplexity, replace_OOVs, LaplaceLM\n",
    "\n",
    "# DELETE - moved to above\n",
    "# # Training data document_plaintext tokenized\n",
    "# document_plaintext_tokenized_bengali = list(df_train_bengali[\"document_plaintext_tokenized\"].explode())\n",
    "# document_plaintext_tokenized_arabic = list(df_train_arabic[\"document_plaintext_tokenized\"].explode())\n",
    "# document_plaintext_tokenized_indonesian = list(df_train_indonesian[\"document_plaintext_tokenized\"].explode())\n",
    "\n",
    "# # Validation data document_plaintext tokenized\n",
    "# document_plaintext_tokenized_val_bengali = list(df_val_bengali[\"document_plaintext_tokenized\"].explode())\n",
    "# document_plaintext_tokenized_val_arabic = list(df_val_arabic[\"document_plaintext_tokenized\"].explode())\n",
    "# document_plaintext_tokenized_val_indonesian = list(df_val_indonesian[\"document_plaintext_tokenized\"].explode())\n",
    "\n",
    "def perplexity_uniform(train, test):\n",
    "    oov_train = inject_OOVs(train)\n",
    "    oov_vocab = set(oov_train)\n",
    "    oov_test = replace_OOVs(oov_vocab, test)\n",
    "    oov_baseline = UniformLM(oov_vocab)\n",
    "    return perplexity(oov_baseline,oov_test)\n",
    "\n",
    "print(\"Bengali perplexity UniformLM:\",perplexity_uniform(document_plaintext_tokenized_bengali, document_plaintext_tokenized_val_bengali))\n",
    "print(\"Arabic perplexity UniformLM:\",perplexity_uniform(document_plaintext_tokenized_arabic, document_plaintext_tokenized_val_arabic))\n",
    "print(\"Indonesian perplexity UniformLM:\",perplexity_uniform(document_plaintext_tokenized_indonesian, document_plaintext_tokenized_val_indonesian))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali perplexity Unigram: 371.19288552880323\n",
      "Arabic perplexity Unigram: 751.8094261847106\n",
      "Indonesian perplexity Unigram: 1735.9898871981518\n"
     ]
    }
   ],
   "source": [
    "def perplexity_ngram(train, test, n):\n",
    "    oov_train = inject_OOVs(train)\n",
    "    oov_vocab = set(oov_train)\n",
    "    oov_test = replace_OOVs(oov_vocab, test)\n",
    "    oov_baseline = NGramLM(oov_train, n)\n",
    "    return perplexity(oov_baseline,oov_test)\n",
    "\n",
    "print(\"Bengali perplexity Unigram:\",perplexity_ngram(document_plaintext_tokenized_bengali, document_plaintext_tokenized_val_bengali, 1))\n",
    "print(\"Arabic perplexity Unigram:\",perplexity_ngram(document_plaintext_tokenized_arabic, document_plaintext_tokenized_val_arabic, 1))\n",
    "print(\"Indonesian perplexity Unigram:\",perplexity_ngram(document_plaintext_tokenized_indonesian, document_plaintext_tokenized_val_indonesian, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali perplexity Unigram with Laplace smoothing: 371.2273172670681\n",
      "Arabic perplexity Unigram with Laplace smoothing: 751.7557916854116\n",
      "Indonesian perplexity Unigram with Laplace smoothing: 1734.9462484780802\n"
     ]
    }
   ],
   "source": [
    "# creating ngram model with laplace smoothing\n",
    "def perplexity_ngram_laplace(train, test, n):\n",
    "    oov_train = inject_OOVs(train)\n",
    "    oov_vocab = set(oov_train)\n",
    "    oov_test = replace_OOVs(oov_vocab, test)\n",
    "    oov_baseline = LaplaceLM(NGramLM(oov_train, n), alpha=0.1)\n",
    "    return perplexity(oov_baseline,oov_test)\n",
    "\n",
    "# first comparing difference with adding laplace smoothing to unigram model\n",
    "print(\"Bengali perplexity Unigram with Laplace smoothing:\",perplexity_ngram_laplace(document_plaintext_tokenized_bengali, document_plaintext_tokenized_val_bengali, 1))\n",
    "print(\"Arabic perplexity Unigram with Laplace smoothing:\",perplexity_ngram_laplace(document_plaintext_tokenized_arabic, document_plaintext_tokenized_val_arabic, 1))\n",
    "print(\"Indonesian perplexity Unigram with Laplace smoothing:\",perplexity_ngram_laplace(document_plaintext_tokenized_indonesian, document_plaintext_tokenized_val_indonesian, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali perplexity Bigram with Laplace smoothing: 89.82380817431472\n",
      "Arabic perplexity Bigram with Laplace smoothing: 175.6236877262895\n",
      "Indonesian perplexity Bigram with Laplace smoothing: 726.1326796894544\n"
     ]
    }
   ],
   "source": [
    "# BiGram with laplace smoothing, no longer has 'inf' bug/error \n",
    "print(\"Bengali perplexity Bigram with Laplace smoothing:\",perplexity_ngram_laplace(document_plaintext_tokenized_bengali, document_plaintext_tokenized_val_bengali, 2))\n",
    "print(\"Arabic perplexity Bigram with Laplace smoothing:\",perplexity_ngram_laplace(document_plaintext_tokenized_arabic, document_plaintext_tokenized_val_arabic, 2))\n",
    "print(\"Indonesian perplexity Bigram with Laplace smoothing:\",perplexity_ngram_laplace(document_plaintext_tokenized_indonesian, document_plaintext_tokenized_val_indonesian, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali perplexity Trigram with Laplace smoothing: 219.5324400940397\n",
      "Arabic perplexity Trigram with Laplace smoothing: 654.1368957845298\n",
      "Indonesian perplexity Trigram with Laplace smoothing: 4166.703512001609\n"
     ]
    }
   ],
   "source": [
    "print(\"Bengali perplexity Trigram with Laplace smoothing:\",perplexity_ngram_laplace(document_plaintext_tokenized_bengali, document_plaintext_tokenized_val_bengali, 3))\n",
    "print(\"Arabic perplexity Trigram with Laplace smoothing:\",perplexity_ngram_laplace(document_plaintext_tokenized_arabic, document_plaintext_tokenized_val_arabic, 3))\n",
    "print(\"Indonesian perplexity Trigram with Laplace smoothing:\",perplexity_ngram_laplace(document_plaintext_tokenized_indonesian, document_plaintext_tokenized_val_indonesian, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, models are created for the question texts for all 5 models as outlined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali question perplexity UniformLM: 694.9999999976678\n",
      "Arabic question perplexity UniformLM: 2578.0000000642076\n",
      "Indonesian question perplexity UniformLM: 5437.999999995055\n"
     ]
    }
   ],
   "source": [
    "print(\"Bengali question perplexity UniformLM:\",perplexity_uniform(question_text_tokenized_bengali, document_plaintext_tokenized_val_bengali))\n",
    "print(\"Arabic question perplexity UniformLM:\",perplexity_uniform(question_text_tokenized_arabic, document_plaintext_tokenized_val_arabic))\n",
    "print(\"Indonesian question perplexity UniformLM:\",perplexity_uniform(question_text_tokenized_indonesian, document_plaintext_tokenized_val_indonesian))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali questions perplexity Unigram: 425.35365857669797\n",
      "Arabic questions perplexity Unigram: 1133.505444711856\n",
      "Indonesian questions perplexity Unigram: 990.4566121951657\n"
     ]
    }
   ],
   "source": [
    "print(\"Bengali questions perplexity Unigram:\",perplexity_ngram(question_text_tokenized_bengali, document_plaintext_tokenized_val_bengali, 1))\n",
    "print(\"Arabic questions perplexity Unigram:\",perplexity_ngram(question_text_tokenized_arabic, document_plaintext_tokenized_val_arabic, 1))\n",
    "print(\"Indonesian questions perplexity Unigram:\",perplexity_ngram(question_text_tokenized_indonesian, document_plaintext_tokenized_val_indonesian, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali questions perplexity Unigram with Laplace smoothing: 424.56145682244716\n",
      "Arabic questions perplexity Unigram with Laplace smoothing: 1129.725617573961\n",
      "Indonesian questions perplexity Unigram with Laplace smoothing: 983.7031816588487\n"
     ]
    }
   ],
   "source": [
    "print(\"Bengali questions perplexity Unigram with Laplace smoothing:\",perplexity_ngram_laplace(question_text_tokenized_bengali, document_plaintext_tokenized_val_bengali, 1))\n",
    "print(\"Arabic questions perplexity Unigram with Laplace smoothing:\",perplexity_ngram_laplace(question_text_tokenized_arabic, document_plaintext_tokenized_val_arabic, 1))\n",
    "print(\"Indonesian questions perplexity Unigram with Laplace smoothing:\",perplexity_ngram_laplace(question_text_tokenized_indonesian, document_plaintext_tokenized_val_indonesian, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali questions perplexity Bigram with Laplace smoothing: 263.3711209698923\n",
      "Arabic questions perplexity Bigram with Laplace smoothing: 775.6720980957562\n",
      "Indonesian questions perplexity Bigram with Laplace smoothing: 1421.3203888280011\n"
     ]
    }
   ],
   "source": [
    "print(\"Bengali questions perplexity Bigram with Laplace smoothing:\",perplexity_ngram_laplace(question_text_tokenized_bengali, document_plaintext_tokenized_val_bengali, 2))\n",
    "print(\"Arabic questions perplexity Bigram with Laplace smoothing:\",perplexity_ngram_laplace(question_text_tokenized_arabic, document_plaintext_tokenized_val_arabic, 2))\n",
    "print(\"Indonesian questions perplexity Bigram with Laplace smoothing:\",perplexity_ngram_laplace(question_text_tokenized_indonesian, document_plaintext_tokenized_val_indonesian, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali questions perplexity Trigram with Laplace smoothing: 347.24875347427394\n",
      "Arabic questions perplexity Trigram with Laplace smoothing: 1537.0735738671785\n",
      "Indonesian questions perplexity Trigram with Laplace smoothing: 3557.7989669057865\n"
     ]
    }
   ],
   "source": [
    "print(\"Bengali questions perplexity Trigram with Laplace smoothing:\",perplexity_ngram_laplace(question_text_tokenized_bengali, document_plaintext_tokenized_val_bengali, 3))\n",
    "print(\"Arabic questions perplexity Trigram with Laplace smoothing:\",perplexity_ngram_laplace(question_text_tokenized_arabic, document_plaintext_tokenized_val_arabic, 3))\n",
    "print(\"Indonesian questions perplexity Trigram with Laplace smoothing:\",perplexity_ngram_laplace(question_text_tokenized_indonesian, document_plaintext_tokenized_val_indonesian, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
